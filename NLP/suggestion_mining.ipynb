{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from numpy import nan\n",
    "from bs4 import BeautifulSoup    \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8499 entries, 0 to 8498\n",
      "Data columns (total 3 columns):\n",
      "663_3                                                                                                                                                                                                                                                                           8499 non-null object\n",
      "\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"    8499 non-null object\n",
      "1                                                                                                                                                                                                                                                                               8499 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 199.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>663_3</th>\n",
       "      <th>\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663_4</td>\n",
       "      <td>\"Note: in your .csproj file, there is a Suppor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>664_1</td>\n",
       "      <td>\"Wich means the new version not fully replaced...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664_2</td>\n",
       "      <td>\"Some of my users will still receive the old x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>664_3</td>\n",
       "      <td>\"The store randomly gives the old xap or the n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664_4</td>\n",
       "      <td>\"My app has a WP7 version and a WP8 version XA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   663_3  \\\n",
       "0  663_4   \n",
       "1  664_1   \n",
       "2  664_2   \n",
       "3  664_3   \n",
       "4  664_4   \n",
       "\n",
       "  \"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"  \\\n",
       "0  \"Note: in your .csproj file, there is a Suppor...                                                                                                                                                                                                                             \n",
       "1  \"Wich means the new version not fully replaced...                                                                                                                                                                                                                             \n",
       "2  \"Some of my users will still receive the old x...                                                                                                                                                                                                                             \n",
       "3  \"The store randomly gives the old xap or the n...                                                                                                                                                                                                                             \n",
       "4  \"My app has a WP7 version and a WP8 version XA...                                                                                                                                                                                                                             \n",
       "\n",
       "   1  \n",
       "0  0  \n",
       "1  0  \n",
       "2  0  \n",
       "3  0  \n",
       "4  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing traindata\n",
    "data = pd.read_csv(r'train.csv',encoding='latin-1')\n",
    "#info about train data\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proprocessing the data by tokenizing, lemmetizing and removing stopwords\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', str(text))\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    word_tokens = word_tokenize(text) \n",
    "    review = [w for w in word_tokens if not w in stop_words] \n",
    "    return (' '.join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text (review) and the corresponding label\n",
    "text = data.iloc[:,1]\n",
    "label = data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the training corpus by performing the preprocessing steps\n",
    "train_corpus= []\n",
    "for i in range(0, 8499):\n",
    "    train_corpus.append(clean_text(text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the training data corpus into the file\n",
    "file =  open(r'train_corpus.txt','w',encoding ='utf-8')\n",
    "for i in train_corpus:\n",
    "    file.write(i+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using CountVectorizing for extracting the features and converting text into sparse matrix\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000)\n",
    "X_train = vectorizer.fit_transform(train_corpus).toarray()\n",
    "y_train = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 592 entries, 0 to 591\n",
      "Data columns (total 3 columns):\n",
      "id          592 non-null object\n",
      "sentence    592 non-null object\n",
      "label       592 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1310_1</td>\n",
       "      <td>I'm not asking Microsoft to Gives permission l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1312_1</td>\n",
       "      <td>somewhere between Android and iPhone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1313_1</td>\n",
       "      <td>And in the Windows Store you can flag the App ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1313_2</td>\n",
       "      <td>Many thanks Sameh Hi, As we know, there is a l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1313_3</td>\n",
       "      <td>The idea is that we can develop a regular app ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                           sentence  label\n",
       "0  1310_1  I'm not asking Microsoft to Gives permission l...      1\n",
       "1  1312_1              somewhere between Android and iPhone.      0\n",
       "2  1313_1  And in the Windows Store you can flag the App ...      0\n",
       "3  1313_2  Many thanks Sameh Hi, As we know, there is a l...      0\n",
       "4  1313_3  The idea is that we can develop a regular app ...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the test data\n",
    "test_data = pd.read_csv(r'test.csv',encoding='latin-1')\n",
    "test_data.info()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000)\n",
    "#performing the preprocessing, extraction of features from test corpus\n",
    "text = test_data['sentence']\n",
    "label1 = test_data['label']\n",
    "test_corpus= []\n",
    "for i in range(0, 592):\n",
    "    test_corpus.append(clean_text(text[i]))\n",
    "file =  open(r'test_corpus.txt','w',encoding ='utf-8')\n",
    "for i in test_corpus:\n",
    "    file.write(i+'\\n')\n",
    "file.close()\n",
    "X_test = vectorizer.fit_transform(test_corpus).toarray()\n",
    "y_test = label1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 832 entries, 0 to 831\n",
      "Data columns (total 3 columns):\n",
      "9566                                          832 non-null int64\n",
      "This would enable live traffic aware apps.    832 non-null object\n",
      "X                                             832 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 19.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#performing the preprocessing, extraction of features from test corpus\n",
    "eval_data = pd.read_csv(r'eval.csv',encoding='latin-1')\n",
    "eval_data.info()\n",
    "eval_data.head()\n",
    "\n",
    "text = eval_data.iloc[:,1]\n",
    "eval_corpus= []\n",
    "for i in range(0, 832):\n",
    "    eval_corpus.append(clean_text(text[i]))\n",
    "file =  open(r'eval_corpus.txt','w',encoding ='utf-8')\n",
    "for i in eval_corpus:\n",
    "    file.write(i+'\\n')\n",
    "file.close()\n",
    "X_eval = vectorizer.fit_transform(eval_corpus).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training f1 Score\n",
      "NB \n",
      "0.7216494845360826\n"
     ]
    }
   ],
   "source": [
    "print('Training f1 Score')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_NB = classifier.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"NB \")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train.toarray(), y_train)\n",
    "y_pred_LDA = classifier.predict(X_train.toarray())\n",
    "\n",
    "\n",
    "print(\"Training f1 Score: LDA\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_LDA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Model - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM\n",
      "0.7216494845360826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_GMM = classifier.predict(X_train)\n",
    "\n",
    "print(\"GMM\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred_GMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-358a126ff660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multinomial NB-  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing f1 Score\n",
      "NB \n",
      "0.18681318681318682\n"
     ]
    }
   ],
   "source": [
    "print('Testing f1 Score')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_NB = classifier.predict(X_test)\n",
    "\n",
    "print(\"NB \")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-10b28f08e0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred_LDA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shrinkage not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_svd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lsqr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_lsqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36m_solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Variables are collinear.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_LDA = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Testing f1 Score: LDA\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred_LDA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM\n",
      "0.18681318681318682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_GMM = classifier.predict(X_test)\n",
    "\n",
    "print(\"GMM\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred_GMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiLayer Perceptron - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training f1 score of MLP\n",
      "0.9819927971188475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "print(\"Training f1 score of MLP\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multilayer Perceptron - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score of MLP\n",
      "0.3211678832116788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Test f1 score of MLP\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "test_data = pd.read_csv(r'test.csv',encoding='latin-1')\n",
    "with open(\"mlp_results.csv\",'w',encoding='utf-8',newline='') as outfile:\n",
    "    mywriter = csv.writer(outfile)\n",
    "    #manually add header\n",
    "    test_data['label'] = y_pred\n",
    "    td = np.array(test_data)\n",
    "    for i in td:\n",
    "        mywriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8499, 1, 2000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM + CNN\n",
    "The network starts with an embedding layer. The layer lets the system expand each token to a more massive vector, allowing the network to represent a word in a meaningful way. The layer takes 20000 as the first argument, which is the size of our vocabulary, and 100 as the second input parameter, which is the dimension of the embedding. The third parameter is the input_length of 50, which is the length of each comment sequence.\n",
    "One way to speed up the training time is to improve the network adding “Convolutional” layer. Convolutional Neural Networks (CNN) come from image processing. They pass a “filter” over the data and calculate a higher-level representation. They have been shown to work surprisingly well for text, even though they have none of the sequence processing ability of LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', error_bad_lines=False)\n",
    "df= df.dropna()\n",
    "df.head()\n",
    "\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df.iloc[:,1])\n",
    "sequences = tokenizer.texts_to_sequences(df.iloc[:,1])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "labels =  df.iloc[:,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv',encoding ='latin-1')\n",
    "df= df.dropna()\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df.iloc[:,1])\n",
    "sequences = tokenizer.texts_to_sequences(df.iloc[:,1])\n",
    "X_test = pad_sequences(sequences, maxlen=50)\n",
    "y_test =  df.iloc[:,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Manually compute f1 score as Keras 2.0 doesnt provide the f1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8499 samples, validate on 592 samples\n",
      "Epoch 1/3\n",
      "8499/8499 [==============================] - 267s 31ms/step - loss: 0.4235 - f1: 0.4014 - val_loss: 1.3056 - val_f1: 0.1242\n",
      "Epoch 2/3\n",
      "8499/8499 [==============================] - 258s 30ms/step - loss: 0.2562 - f1: 0.7712 - val_loss: 1.3040 - val_f1: 0.2481\n",
      "Epoch 3/3\n",
      "8499/8499 [==============================] - 260s 31ms/step - loss: 0.1803 - f1: 0.8493 - val_loss: 1.7951 - val_f1: 0.2283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5dc70c8208>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=[f1])\n",
    "    return model_conv\n",
    "model_conv = create_conv_model()\n",
    "model_conv.fit(data, np.array(labels), validation_data=(X_test,y_test), epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8499 samples, validate on 592 samples\n",
      "Epoch 1/10\n",
      "8499/8499 [==============================] - 287s 34ms/step - loss: 0.4281 - f1: 0.3895 - val_loss: 1.2043 - val_f1: 0.1913\n",
      "Epoch 2/10\n",
      "8499/8499 [==============================] - 275s 32ms/step - loss: 0.2546 - f1: 0.7760 - val_loss: 1.4256 - val_f1: 0.1731\n",
      "Epoch 3/10\n",
      "8499/8499 [==============================] - 271s 32ms/step - loss: 0.1766 - f1: 0.8490 - val_loss: 1.5572 - val_f1: 0.2162\n",
      "Epoch 4/10\n",
      "8499/8499 [==============================] - 291s 34ms/step - loss: 0.1224 - f1: 0.9065 - val_loss: 2.5976 - val_f1: 0.1556\n",
      "Epoch 5/10\n",
      "8499/8499 [==============================] - 300s 35ms/step - loss: 0.0882 - f1: 0.9352 - val_loss: 2.1904 - val_f1: 0.2176\n",
      "Epoch 6/10\n",
      "8499/8499 [==============================] - 285s 34ms/step - loss: 0.0650 - f1: 0.9552 - val_loss: 2.5242 - val_f1: 0.2296\n",
      "Epoch 7/10\n",
      "8499/8499 [==============================] - 274s 32ms/step - loss: 0.0515 - f1: 0.9625 - val_loss: 2.4472 - val_f1: 0.3011\n",
      "Epoch 8/10\n",
      "8499/8499 [==============================] - 293s 34ms/step - loss: 0.0446 - f1: 0.9648 - val_loss: 3.2777 - val_f1: 0.2226\n",
      "Epoch 9/10\n",
      "8499/8499 [==============================] - 284s 33ms/step - loss: 0.0347 - f1: 0.9721 - val_loss: 3.3586 - val_f1: 0.2208\n",
      "Epoch 10/10\n",
      "8499/8499 [==============================] - 274s 32ms/step - loss: 0.0309 - f1: 0.9725 - val_loss: 3.2229 - val_f1: 0.2889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5db84aae80>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=[f1])\n",
    "    return model_conv\n",
    "model_conv = create_conv_model()\n",
    "model_conv.fit(data, np.array(labels), validation_data=(X_test,y_test), epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS:\n",
    "\n",
    "        MODEL              TRAINING f1 Score    Testing f1 Score\n",
    "        MulinomialNB         0.7216               0.1868\n",
    "        GMM                  0.7216               0.1868\n",
    "        MLP                  0.9819               0.3211\n",
    "        LSTM+CNN(3 epochs)   0.8493               0.2283\n",
    "        LSTM+CNN(10 epochs)  0.9725               0.2889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
